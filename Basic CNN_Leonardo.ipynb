{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os, shutil\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of categories that will be considered. The items of this list have the same name as the names of the folders \n",
    "#the images are strored in.\n",
    "\n",
    "list_categories = ['bar','bowling','buffet','casino','concert_hall','fastfood_restaurant','gameroom','gym',\n",
    "                  'hairsalon','movietheater','restaurant','airport_inside','church_inside','cloister','elevator',\n",
    "                  'florist','inside_bus','library','locker_room','museum','poolinside','prisoncell','subway',\n",
    "                   'trainstation','waitingroom']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we create the file structure to split the images in train, validation and test sets.\n",
    "\n",
    "\n",
    "#Where the original images are stored.\n",
    "original_dataset_dir = 'original_images'\n",
    "\n",
    "#Where we will store the images separated by train, validation and test sets.\n",
    "base_dir = 'images'\n",
    "\n",
    "os.mkdir(base_dir)\n",
    "\n",
    "train_dir = os.path.join(base_dir,'train')\n",
    "os.mkdir(train_dir)\n",
    "\n",
    "validation_dir = os.path.join(base_dir,'validation')\n",
    "os.mkdir(validation_dir)\n",
    "\n",
    "test_dir = os.path.join(base_dir,'test')\n",
    "os.mkdir(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inside each of the folders created peveously, we create a folder for each of the categories.\n",
    "\n",
    "or i in range(len(list_categories)):\n",
    "    os.mkdir(os.path.join(train_dir,list_categories[i]))\n",
    "    \n",
    "for i in range(len(list_categories)):\n",
    "    os.mkdir(os.path.join(validation_dir,list_categories[i]))\n",
    "    \n",
    "for i in range(len(list_categories)):\n",
    "    os.mkdir(os.path.join(test_dir,list_categories[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next we divide the images into train(60%), validation(20%) and test(20%) and copy them to the new file structure.\n",
    "\n",
    "for i in range(len(list_categories)):\n",
    "    directory = os.path.join(original_dataset_dir,list_categories[i])\n",
    "    number_of_files = sum(1 for item in os.listdir(directory) if os.path.isfile(os.path.join(directory, item)))\n",
    "    train_indice = math.ceil(number_of_files*0.6)\n",
    "    validation_indice = math.ceil(number_of_files*0.8)\n",
    "    iterator = 0\n",
    "    for file in os.listdir(directory):\n",
    "        if iterator < train_indice:\n",
    "            src = os.path.join(directory,file)\n",
    "            dst = os.path.join(train_dir,list_categories[i],file)\n",
    "            shutil.copyfile(src,dst)\n",
    "        elif iterator < validation_indice:\n",
    "            src = os.path.join(directory,file)\n",
    "            dst = os.path.join(validation_dir,list_categories[i],file)\n",
    "            shutil.copyfile(src,dst)\n",
    "        else:\n",
    "            src = os.path.join(directory,file)\n",
    "            dst = os.path.join(test_dir,list_categories[i],file)\n",
    "            shutil.copyfile(src,dst)\n",
    "        iterator += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model definition\n",
    "\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Conv2D(32, (3,3), activation='relu', input_shape=(150,150,3)))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "\n",
    "model.add(layers.Conv2D(64, (3,3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "\n",
    "model.add(layers.Conv2D(128, (3,3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "\n",
    "model.add(layers.Conv2D(128, (3,3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "          \n",
    "model.add(layers.Flatten())\n",
    "          \n",
    "model.add(layers.Dense(512,activation='relu'))\n",
    "\n",
    "model.add(layers.Dense(25,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Compilation\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "             metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3492 images belonging to 25 classes.\n",
      "Found 1163 images belonging to 25 classes.\n"
     ]
    }
   ],
   "source": [
    "#Data genaration for the training and validation steps\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(150,150),\n",
    "        batch_size=20,\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(150,150),\n",
    "        batch_size=20,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data batch shape:  (20, 150, 150, 3)\n",
      "labels batch shape:  (20, 25)\n"
     ]
    }
   ],
   "source": [
    "for data_batch, labels_batch in train_generator:\n",
    "    print('data batch shape: ', data_batch.shape)\n",
    "    print('labels batch shape: ', labels_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "100/100 [==============================] - 40s 400ms/step - loss: 3.0072 - acc: 0.1029 - val_loss: 2.9989 - val_acc: 0.1060\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 47s 466ms/step - loss: 2.8663 - acc: 0.1095 - val_loss: 3.4432 - val_acc: 0.1170\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 44s 444ms/step - loss: 2.7152 - acc: 0.1606 - val_loss: 2.8297 - val_acc: 0.1913\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 46s 460ms/step - loss: 2.6374 - acc: 0.2025 - val_loss: 2.9245 - val_acc: 0.2350\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 46s 459ms/step - loss: 2.4979 - acc: 0.2324 - val_loss: 2.5471 - val_acc: 0.2055\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 47s 465ms/step - loss: 2.4455 - acc: 0.2580 - val_loss: 2.6428 - val_acc: 0.2289\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 51s 510ms/step - loss: 2.3873 - acc: 0.2725 - val_loss: 2.2681 - val_acc: 0.2520\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 53s 529ms/step - loss: 2.2557 - acc: 0.3015 - val_loss: 2.0548 - val_acc: 0.2350\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 52s 520ms/step - loss: 2.2489 - acc: 0.3170 - val_loss: 3.1270 - val_acc: 0.2258\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 54s 544ms/step - loss: 2.1314 - acc: 0.3390 - val_loss: 2.2845 - val_acc: 0.2584\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 53s 530ms/step - loss: 2.0536 - acc: 0.3660 - val_loss: 3.0499 - val_acc: 0.2340\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 52s 518ms/step - loss: 2.0149 - acc: 0.3805 - val_loss: 2.5100 - val_acc: 0.3001\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 51s 507ms/step - loss: 1.8818 - acc: 0.4105 - val_loss: 2.3414 - val_acc: 0.2523\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 52s 519ms/step - loss: 1.8738 - acc: 0.4287 - val_loss: 2.4822 - val_acc: 0.2720\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 53s 526ms/step - loss: 1.7253 - acc: 0.4709 - val_loss: 2.4081 - val_acc: 0.2899\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 53s 534ms/step - loss: 1.6938 - acc: 0.4745 - val_loss: 2.5769 - val_acc: 0.2747\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 51s 514ms/step - loss: 1.5711 - acc: 0.5120 - val_loss: 2.0494 - val_acc: 0.3001\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 52s 518ms/step - loss: 1.5047 - acc: 0.5235 - val_loss: 3.1984 - val_acc: 0.2635\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 52s 518ms/step - loss: 1.4321 - acc: 0.5517 - val_loss: 3.2396 - val_acc: 0.2920\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 51s 515ms/step - loss: 1.2852 - acc: 0.6000 - val_loss: 2.1707 - val_acc: 0.3050\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 52s 524ms/step - loss: 1.2900 - acc: 0.5989 - val_loss: 1.9355 - val_acc: 0.2991\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 51s 510ms/step - loss: 1.1169 - acc: 0.6506 - val_loss: 2.3545 - val_acc: 0.2889\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 53s 533ms/step - loss: 1.0735 - acc: 0.6620 - val_loss: 3.2997 - val_acc: 0.2970\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 56s 562ms/step - loss: 0.9312 - acc: 0.6988 - val_loss: 1.9692 - val_acc: 0.3042\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 55s 552ms/step - loss: 0.8628 - acc: 0.7295 - val_loss: 3.5870 - val_acc: 0.3316\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 55s 549ms/step - loss: 0.7948 - acc: 0.7470 - val_loss: 2.3827 - val_acc: 0.2991\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 55s 546ms/step - loss: 0.6791 - acc: 0.7796 - val_loss: 3.3837 - val_acc: 0.2880\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 56s 563ms/step - loss: 0.6638 - acc: 0.7922 - val_loss: 2.8219 - val_acc: 0.3123\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 55s 552ms/step - loss: 0.4804 - acc: 0.8555 - val_loss: 3.6197 - val_acc: 0.2920\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 53s 531ms/step - loss: 0.4912 - acc: 0.8504 - val_loss: 4.1712 - val_acc: 0.2970\n"
     ]
    }
   ],
   "source": [
    "#Fitting the model:\n",
    "\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=100,\n",
    "    epochs=30,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1150 images belonging to 25 classes.\n"
     ]
    }
   ],
   "source": [
    "#Data generation for the test step:\n",
    "\n",
    "test_set = test_datagen.flow_from_directory(test_dir,\n",
    "                                            target_size = (150, 150),\n",
    "                                            batch_size = 20,\n",
    "                                            class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.796738862991333, 0.3199999928474426]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluate model\n",
    "\n",
    "model.evaluate_generator(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
