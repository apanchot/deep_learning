{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os, shutil\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of categories that will be considered. The items of this list have the same name as the names of the folders \n",
    "#the images are strored in.\n",
    "\n",
    "list_categories = ['bar','bowling','buffet','casino','concert_hall','fastfood_restaurant','gameroom','gym',\n",
    "                  'hairsalon','movietheater','restaurant','airport_inside','church_inside','cloister','elevator',\n",
    "                  'florist','inside_bus','library','locker_room','museum','poolinside','prisoncell','subway',\n",
    "                   'trainstation','waitingroom']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we create the file structure to split the images in train, validation and test sets.\n",
    "\n",
    "\n",
    "#Where the original images are stored.\n",
    "original_dataset_dir = 'original_images'\n",
    "\n",
    "#Where we will store the images separated by train, validation and test sets.\n",
    "base_dir = 'images'\n",
    "\n",
    "os.mkdir(base_dir)\n",
    "\n",
    "train_dir = os.path.join(base_dir,'train')\n",
    "os.mkdir(train_dir)\n",
    "\n",
    "validation_dir = os.path.join(base_dir,'validation')\n",
    "os.mkdir(validation_dir)\n",
    "\n",
    "test_dir = os.path.join(base_dir,'test')\n",
    "os.mkdir(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inside each of the folders created peveously, we create a folder for each of the categories.\n",
    "\n",
    "for i in range(len(list_categories)):\n",
    "    os.mkdir(os.path.join(train_dir,list_categories[i]))\n",
    "    \n",
    "for i in range(len(list_categories)):\n",
    "    os.mkdir(os.path.join(validation_dir,list_categories[i]))\n",
    "    \n",
    "for i in range(len(list_categories)):\n",
    "    os.mkdir(os.path.join(test_dir,list_categories[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next we divide the images into train(60%), validation(20%) and test(20%) and copy them to the new file structure.\n",
    "\n",
    "for i in range(len(list_categories)):\n",
    "    directory = os.path.join(original_dataset_dir,list_categories[i])\n",
    "    number_of_files = sum(1 for item in os.listdir(directory) if os.path.isfile(os.path.join(directory, item)))\n",
    "    train_indice = math.ceil(number_of_files*0.6)\n",
    "    validation_indice = math.ceil(number_of_files*0.8)\n",
    "    iterator = 0\n",
    "    for file in os.listdir(directory):\n",
    "        if iterator < train_indice:\n",
    "            src = os.path.join(directory,file)\n",
    "            dst = os.path.join(train_dir,list_categories[i],file)\n",
    "            shutil.copyfile(src,dst)\n",
    "        elif iterator < validation_indice:\n",
    "            src = os.path.join(directory,file)\n",
    "            dst = os.path.join(validation_dir,list_categories[i],file)\n",
    "            shutil.copyfile(src,dst)\n",
    "        else:\n",
    "            src = os.path.join(directory,file)\n",
    "            dst = os.path.join(test_dir,list_categories[i],file)\n",
    "            shutil.copyfile(src,dst)\n",
    "        iterator += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model definition\n",
    "\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Conv2D(32, (3,3), activation='relu', input_shape=(150,150,3)))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "\n",
    "model.add(layers.Conv2D(64, (3,3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "\n",
    "model.add(layers.Conv2D(128, (3,3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "\n",
    "model.add(layers.Conv2D(128, (3,3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "          \n",
    "model.add(layers.Flatten())\n",
    "          \n",
    "model.add(layers.Dense(512,activation='relu'))\n",
    "\n",
    "model.add(layers.Dense(25,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Compilation\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "             metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3492 images belonging to 25 classes.\n",
      "Found 1163 images belonging to 25 classes.\n"
     ]
    }
   ],
   "source": [
    "#Data genaration for the training and validation steps\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(150,150),\n",
    "        batch_size=20,\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(150,150),\n",
    "        batch_size=20,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data batch shape:  (20, 150, 150, 3)\n",
      "labels batch shape:  (20, 25)\n"
     ]
    }
   ],
   "source": [
    "for data_batch, labels_batch in train_generator:\n",
    "    print('data batch shape: ', data_batch.shape)\n",
    "    print('labels batch shape: ', labels_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "100/100 [==============================] - 43s 426ms/step - loss: 3.0121 - acc: 0.1089 - val_loss: 2.8072 - val_acc: 0.1230\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 2.8774 - acc: 0.1365 - val_loss: 3.0368 - val_acc: 0.1394\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 47s 475ms/step - loss: 2.7284 - acc: 0.1840 - val_loss: 2.5544 - val_acc: 0.1933\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 47s 466ms/step - loss: 2.6018 - acc: 0.2088 - val_loss: 3.1182 - val_acc: 0.2391\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 49s 486ms/step - loss: 2.5406 - acc: 0.2210 - val_loss: 2.8168 - val_acc: 0.1984\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 50s 497ms/step - loss: 2.4476 - acc: 0.2490 - val_loss: 2.6245 - val_acc: 0.2319\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 51s 512ms/step - loss: 2.4297 - acc: 0.2480 - val_loss: 2.4121 - val_acc: 0.2130\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 52s 524ms/step - loss: 2.3196 - acc: 0.2765 - val_loss: 2.3298 - val_acc: 0.2523\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 52s 515ms/step - loss: 2.2245 - acc: 0.3047 - val_loss: 2.2676 - val_acc: 0.2655\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 53s 531ms/step - loss: 2.2268 - acc: 0.3002 - val_loss: 2.2758 - val_acc: 0.2950\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 52s 518ms/step - loss: 2.0714 - acc: 0.3459 - val_loss: 2.0772 - val_acc: 0.2330\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 54s 539ms/step - loss: 2.0539 - acc: 0.3645 - val_loss: 2.5057 - val_acc: 0.2777\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 52s 524ms/step - loss: 1.9658 - acc: 0.3885 - val_loss: 3.1250 - val_acc: 0.2492\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 51s 515ms/step - loss: 1.9301 - acc: 0.3971 - val_loss: 2.5910 - val_acc: 0.3030\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 53s 530ms/step - loss: 1.7848 - acc: 0.4463 - val_loss: 2.2219 - val_acc: 0.2564\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 53s 526ms/step - loss: 1.7883 - acc: 0.4295 - val_loss: 2.4740 - val_acc: 0.2177\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 52s 523ms/step - loss: 1.6557 - acc: 0.4799 - val_loss: 2.0907 - val_acc: 0.3123\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 53s 528ms/step - loss: 1.5599 - acc: 0.5225 - val_loss: 2.5295 - val_acc: 0.3133\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 50s 496ms/step - loss: 1.5376 - acc: 0.5105 - val_loss: 2.1106 - val_acc: 0.2960\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 50s 498ms/step - loss: 1.4125 - acc: 0.5612 - val_loss: 2.7937 - val_acc: 0.2880\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 50s 502ms/step - loss: 1.4313 - acc: 0.5555 - val_loss: 2.5904 - val_acc: 0.3093\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 50s 497ms/step - loss: 1.2543 - acc: 0.6120 - val_loss: 2.7332 - val_acc: 0.2950\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 50s 498ms/step - loss: 1.2066 - acc: 0.6150 - val_loss: 3.6536 - val_acc: 0.3082\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 52s 519ms/step - loss: 1.0644 - acc: 0.6580 - val_loss: 2.6614 - val_acc: 0.2879\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 53s 528ms/step - loss: 1.0130 - acc: 0.6898 - val_loss: 1.7613 - val_acc: 0.2920\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 55s 548ms/step - loss: 0.9220 - acc: 0.7033 - val_loss: 3.4518 - val_acc: 0.2889\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 54s 541ms/step - loss: 0.8456 - acc: 0.7420 - val_loss: 2.6555 - val_acc: 0.2910\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 55s 548ms/step - loss: 0.8140 - acc: 0.7415 - val_loss: 2.1193 - val_acc: 0.2981\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 62s 622ms/step - loss: 0.6047 - acc: 0.8145 - val_loss: 4.3690 - val_acc: 0.3133\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 62s 617ms/step - loss: 0.6359 - acc: 0.8062 - val_loss: 4.0057 - val_acc: 0.3316\n"
     ]
    }
   ],
   "source": [
    "#Fitting the model:\n",
    "\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=100,\n",
    "    epochs=30,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1150 images belonging to 25 classes.\n"
     ]
    }
   ],
   "source": [
    "#Data generation for the test step:\n",
    "\n",
    "test_set = test_datagen.flow_from_directory(test_dir,\n",
    "                                            target_size = (150, 150),\n",
    "                                            batch_size = 20,\n",
    "                                            class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.6896119117736816, 0.32956522703170776]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluate model\n",
    "\n",
    "model.evaluate_generator(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
